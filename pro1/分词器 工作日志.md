# 分词 工作日志



NLTK是一个高效的Python构建的平台，用来处理自然语言数据，它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如WordNet），还有一套用于分类、标记化、词干标记、解析和语义推理的文本处理库。NLTK可以在Windows、Mac OS以及Linux系统上使用。



Reference 

https://github.com/nltk/nltk_data

https://www.jianshu.com/p/9d232e4a3c28

https://blog.csdn.net/weixin_44633882/article/details/104494276



### 1.安装NLTK

使用`pip install nltk`命令安装NLTK库，NLTK中集成了语料与模型等的包管理器，通过在python解释器中执行以下代码

```python
import nltk
```

代码

```python
import nltk

#先分句再分词
sents = nltk.sent_tokenize("And now for something completely different. I love you.")
word = []
for sent in sents:
    word.append(nltk.word_tokenize(sent))
print(word)

#分词
text = nltk.word_tokenize("And now for something completely different.")
print(text)
#词性标注
tagged = nltk.pos_tag(text)
print (tagged[0:6])
#命名实体识别
entities = nltk.chunk.ne_chunk(tagged)
print (entities)
```

报错

```python
Traceback (most recent call last):
  File "/Users/a123/Desktop/pro1/new.py", line 4, in <module>
    sents = nltk.sent_tokenize("And now for something completely different. I love you.")
  File "/usr/local/lib/python3.9/site-packages/nltk/tokenize/__init__.py", line 106, in sent_tokenize
    tokenizer = load(f"tokenizers/punkt/{language}.pickle")
  File "/usr/local/lib/python3.9/site-packages/nltk/data.py", line 750, in load
    opened_resource = _open(resource_url)
  File "/usr/local/lib/python3.9/site-packages/nltk/data.py", line 876, in _open
    return find(path_, path + [""]).open()
  File "/usr/local/lib/python3.9/site-packages/nltk/data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('punkt')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load tokenizers/punkt/PY3/english.pickle

  Searched in:
    - '/Users/a123/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************
```



因为国内特殊网络原因，无法完成下列语句

```python
  >>> import nltk
  >>> nltk.download('punkt')
```

直接访问`https://github.com/nltk/nltk_data`，手动下载安装到

`'/Users/a123/nltk_data'`文件夹中



重新运行

```bash
[['And', 'now', 'for', 'something', 'completely', 'different', '.'], ['I', 'love', 'you', '.']]
['And', 'now', 'for', 'something', 'completely', 'different', '.']
Traceback (most recent call last):
  File "/Users/a123/Desktop/pro1/new.py", line 14, in <module>
    tagged = nltk.pos_tag(text)
  File "/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py", line 165, in pos_tag
    tagger = _get_tagger(lang)
  File "/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py", line 167, in __init__
    find("taggers/averaged_perceptron_tagger/" + PICKLE)
  File "/usr/local/lib/python3.9/site-packages/nltk/data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource averaged_perceptron_tagger not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle

  Searched in:
    - '/Users/a123/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************
```

直接运行

```
  >>> nltk.download('averaged_perceptron_tagger')
```

长时间未响应



这里不行

【

重新手动下载，拷贝`taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle`

`git clone https://github.com/nltk/nltk_data.git`

】



重新拷贝进taggers

```bash
[['And', 'now', 'for', 'something', 'completely', 'different', '.'], ['I', 'love', 'you', '.']]
['And', 'now', 'for', 'something', 'completely', 'different', '.']
[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]
Traceback (most recent call last):
  File "/Users/a123/Desktop/pro1/new.py", line 17, in <module>
    entities = nltk.chunk.ne_chunk(tagged)
  File "/usr/local/lib/python3.9/site-packages/nltk/chunk/__init__.py", line 183, in ne_chunk
    chunker = load(chunker_pickle)
  File "/usr/local/lib/python3.9/site-packages/nltk/data.py", line 750, in load
    opened_resource = _open(resource_url)
  File "/usr/local/lib/python3.9/site-packages/nltk/data.py", line 876, in _open
    return find(path_, path + [""]).open()
  File "/usr/local/lib/python3.9/site-packages/nltk/data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource maxent_ne_chunker not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('maxent_ne_chunker')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load chunkers/maxent_ne_chunker/PY3/english_ace_multiclass.pickle

  Searched in:
    - '/Users/a123/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'
    - '/usr/local/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************
```



chunker 是**命名实体识别**所以就算了，这个proj用不到

词性标注也并不必要



```python
import nltk
from nltk.book import *

# # 打开文件
# f = open("D:\\App\\test.txt","r")
# text = ""
# line = f.readline()
# while line:
#     text += line
#     line = f.readline()
# f.close()
# text1 = nltk.word_tokenize(text)
# 或者
text1 = nltk.word_tokenize("And now for something completely different. I love you. This is my friend. You are my friend.")

# FreqDist()获取在文本中每个出现的标识符的频率分布
fdist = FreqDist(text1)
print(fdist)
# 词数量
print(fdist.N())
# 不重复词的数量
print(fdist.B())
```



### 2. 爬莎士比亚

安装了一下requests_html库，之前用csv格式，现在直接用文本输出

```python

from requests_html import HTMLSession
import csv

session = HTMLSession()

file = open('hamlet', 'w', newline='')
#输入储存对应文件名

```

网址在http://shakespeare.mit.edu/hamlet/full.html



css selector找了很久，最后就是用了blockquote（所以标题和说话标记的人名都没有爬进去，只有说出来的人名才进去了）

```python
r = session.get('http://shakespeare.mit.edu/hamlet/full.html')
s_texts = r.html.find('blockquote')
for s_text in s_texts:
    #print(s_text.text)
    file.write(s_text.text)
    file.write(' ')

file.close()
```



出一个hamlet文件belike

```
FRANCISCO at his post. Enter to him BERNARDO Who's there? Nay, answer me: stand, and unfold yourself. Long live the king! Bernardo? He. You come most carefully upon your hour. 'Tis now struck twelve; get thee to bed, Francisco. For this relief much thanks: 'tis bitter cold,
And I am sick at heart. Have you had quiet guard? Not a mouse stirring. Well, good night.
If you do meet Horatio and Marcellus,
The rivals of my watch, bid them make haste. I think I hear them. Stand, ho! Who's there?

Enter HORATIO and MARCELLUS Friends to this ground. And liegemen to the Dane. Give you good night. O, farewell, honest soldier:
Who hath relieved you? Bernardo has my place.
Give you good night.

Exit Holla! Bernardo! Say,
```



### 3.分词

```python
import nltk
from nltk.book import *



# # 打开文件
f = open("hamlet","r")
text = ""
line = f.readline()
while line:
    text += line
    line = f.readline()
f.close()
text1 = nltk.word_tokenize(text)
# 或者
#text1 = nltk.word_tokenize("And now for something completely different. I love you. This is my friend. You are my friend.")

f = open("hamlet_divided","w")
for word in text1:
    f.write(word+"\n")
# FreqDist()获取在文本中每个出现的标识符的频率分布
fdist = FreqDist(text1)
print(fdist)
# 词数量
print(fdist.N())
# 不重复词的
print(fdist.B())
```

写进hamlet_divided中， 并且输出两行数字，分别标识词的数量和不重复词的数量。

belike

```
FRANCISCO
at
his
post
.
Enter
to
him
BERNARDO
Who
's
there
?
Nay
,
answer
me
:
stand
```

未去标点、未处理大小写



```
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

new_stop_words = [',','.','?','\'s','!',';','\'t',':','\'d','--','n\'t',
'[','\'ll','\'',']']
for i in new_stop_words:
    stop_words.add(i)
    
filtered_sentence = [w for w in text1 if not w.lower() in stop_words]

```

以上是处理stopwords







```
from nltk.stem import PorterStemmer

ps = PorterStemmer()

……
    for word in filtered_sentence:
        # print(ps.stem(word))
        f.write(ps.stem(word)+"\n")
```

以上是处理wordstemming的

